Key Steps:
    1. Data Ingestion
    2. Data Transformation
    3. Model Trainer
    4. Model Evaluation
    5. Model Deployment

- To begin with, we need to create a local and Github repository for the project

- To create a separate environment for this project, such that:
    - all the packages, dependencies, etc. are installed/mentioned in this folder itself
    - everything required in this project is present in this folder itself, 
    - and we can execute this project from this folder itself
    - GOOD PRACTICE!

    - Make sure you've "Anaconda" installed already
        > WHY? we'll use `conda` for managing environments and packages
    - execute the cmd: "conda create -p venv python==3.8 -y"
    - then, activate this environment: "conda activate ./venv"

    - now, connect the GitHub repo to this Local repo

- now create a "setup.py" & "requirements.txt"
    - "setup.py" 
        -> Allows you to package your project, making it easy to distribute and install.
        -> useful if you plan to share your project with others or deploy it to a production environment.
        -> think of it as "package.json" in JS 
        -> contains the metadata information of the project

    - how will this "setup.py" find the packages?
        - whenever "find_packages" suns, it will search for folders which have "__init__.py"
        - whichever folder has this file (ex: "src"), it will consider it as a package and will BUILD it

- IMP: we can EITHER 
    > 1. run the "setup.py" directly.
    OR 
    > 2. when we run the "requirements.txt" (which is usually considered doing), we need the "setup.py" file to execute simultaneously as well, in-order to build packages"

    - so, for "2.", we add a line "-e ." at the end of requirements.txt file, which will automatically trigger the "setup.py" file.

- now, run "pip install -r requirements.txt"
    -> if a "generic_ml_project.egg-info" folder is created.....SETUP.PY FILE WAS EXECUTED AS WELL!

Project Structure:
    - now, create a "components" folder inside "src" folder
        > don't forget to create "__init__.py" inside this "components" folder as well
            > WHY?  so that components can be exported/imported to some other file location

    - Data Ingestion:
        > reading the dataset from a database OR any other file locations

    - Data Transformation:
        > to change categorical features to numerical features
        > to handle "one hot encoding", "label encoding"

    - Model Trainer:
        > which types of model to use
        > compute confusion matrix/r2 squared error, etc
        > Model Pusher: create & push the "pickle" file into Cloud

    - "Pipeline":
        > Two types: "Training pipeline" & "Prediction pipeline"

        > from the "training_pipeline.py" we'll try to trigger/call all the "components" (data ingestion, transformation, trainer, etc...)

        > once the model is created, we'll use "prediction_pipeline.py" to use the model to predict on NEW DATA

        > + "__init__.py" file

    - now, within "src" folder,
        - create "logger.py" for Logging, 
        - create "exception.py" for Exception Handling, 
        - & create "utils.py" for writing any common functions that will be used accross entire application

- Exploratory Data Analysis (EDA)
    - prefer doing it in Jupyter Notebook (not in .py file)
    - whenever perform EDA, OBSERVATION is SUPER IMP.
    - provide that observation to the stakeholder
    - for each step in project, have a Reason

    - Steps:
        1. Importing Libraries
        2. Loading the Dataset
        3. Understanding the Data
        4. Data Cleaning
        5. Data Visualization
        6. Feature Engineering
        7. Data Preprocessing for Machine Learning Models

    - Detailed:
        1. Importing Libraries
            _> import numpy as np
            _> import pandas as pd
            _> import matplotlib.pyplot as plt
            _> import seaborn as sns

        2. Loading the Dataset
            _> data = pd.read_csv('path_to_your_dataset.csv')

        3. Understanding the Data
            - View the first few rows:
                _> data.head()

            - Get a summary of the dataset:
                _> data.info()
                _> data.describe()

            - Identify missing values:
                _> data.isna().sum()

        4. Data Cleaning
            - Handling missing values: (removing them or filling them with a value)
                _> data.dropna(inplace=True)  # To remove rows with missing values
                _> data.fillna(value, inplace=True)  # To fill missing values

            - Remove duplicate rows:
                _> data.drop_duplicates(inplace=True)

        5. Data Visualization
            - Histogram: To view the distribution of a single feature
                _> data['feature_name'].hist()
                _> sns.boxplot(x='feature_name', data=data)
                _> plt.scatter(data['feature1'], data['feature2'])
                _> sns.heatmap(data.corr(), annot=True)

- Model Training
    - Select One feature of the dataset to be "predicted", based on the rest of the features (used as "inputs")

    - Feature Engineering:
        > When there are 'less no. of features & also less no. of categories inside the features' -> use "One-Hot Encoding"

        > Suppose a feature had "many more no. of categories" -> use "Target Guided Ordinal Encoding"

        > Once the categorical features are converted into numerical features, we perform "Standardization or normalization

    - Feature Scaling:
        > Standardize or normalize features -> using "sklearn.preprocessing"

        > NOTE: Only Numerical Features need to be Normalized
            & Categorical Features need to be "Encoded" into numerical form (NO NEED TO NORMALIZE AFTER THAT)

    - Train_Test_Split:
        > split the X,y data into training/testing dataset (80/20 ratio or anything suitable)

    - Select a Model that gives the best results

    - Train the model (initialize + fit + predict) on the train/test data

    - Evaluate the model (calculate "evaluation metrics")

- Modular Coding 
    - use of ".py" files INSTEAD OF ".ipynb" files
    - first we'll read data from local dataset
    - then, as we proceed, we will read the data from databases

    - Data Ingestion:
        > to read the data and split it into train/test

        > whenever we perform, we require some "inputs"
            > where to save training path/train data
            > where to save test data
            > where to save raw data

        > These inputs we'll be creating in another "class" -> "DataIngestionConfig"
            > We can define this class in another folder "config" (but to keep it simple, let it be)

            > IMP: RECOMMENDED: use "@dataclass" when only to define variables,
                if you want to define functions too, use normal Class (with __init__ function)
                    -> ex: "DataIngestion"

            
    - Data Transformation:
        > to perform Feature Engineering, Data Cleaning
            > convert categorical feat. to numerical form

        
        > Key Components:

            1. Numerical Pipeline:
            - SimpleImputer(strategy="median"): Fills missing numerical values with the median.
            - StandardScaler(): Standardizes numerical features by removing the mean and scaling to unit variance.

            2. Categorical Pipeline:
            - SimpleImputer(strategy="most_frequent"): Fills missing categorical values with the most frequent value.
            - OneHotEncoder(): Converts categorical features into one-hot encoded format.
            - StandardScaler(with_mean=False): Scales one-hot encoded features (mean=False is used for sparse matrices).

            3. ColumnTransformer:
            - Combines the numerical and categorical pipelines, applying them to respective feature subsets.

        
        > PROCESS:
            1. Define Pipelines:
            - Create numerical and categorical pipelines using 'Pipeline'.

            2. Combine Pipelines:
            - Use 'ColumnTransformer' to combine numerical and categorical pipelines.

            3. Fit and Transform:
            - "fit_transform" (training data): 
                - Fits the transformer (learns parameters: mean, std deviation, encoding categories).
                - Transforms the data based on learned parameters.
            - "transform" (test data): 
                - Transforms the test data using parameters learned from training data.

            4. Combine Transformed Data with Target:
            - Use 'np.c_' to concatenate transformed input features with the target feature.


        > IMP NOTE:
            - "fit_transform": Used only on training data to learn and apply transformations.
            - "transform": Used on test data to apply learned transformations from the training data.

    - Model Trainer:
        > to train and evaluate various regression models, and save the best model.

        > Implementation:
            - "ModelTrainerConfig" class: Defines path to save the trained model using '@dataclass'.
            - "ModelTrainer" class:
                - "initiate_model_trainer" method:
                    - split processed dataset into input features (X) and target (y) for both train and test sets.
                    - define a dictionary of regression models to train.
                    - evaluates models using a custom 'evaluate_model' function.
                    - select the best model based on performance metrics.
                    - save the best model using 'save_obj'.
                    - predict on test data and calculates RÂ² score to evaluate model performance.
            
    - Overall Workflow:
        > Data Ingestion: Splits the dataset into training and testing sets (input + target).

        > Data Transformation: 
            - Processes input features (numeric + categorical) using defined pipelines.
            - Concatenates processed input features with target feature to form 'train_arr' & 'test_arr'.

        > Model Training:
            - Accesses the full processed dataset.
            - Trains and evaluates multiple models.
            - Saves the best performing model.

    - HYPERPARAMETER TUNING:
        > control the learning process of a ML model.
        > set before the training process of a model begins.
        > NOTE: they control the learning process itself, rather than being learned from the data.
            - ex: learning rate, no. of neurons in a NeuralNet, kernel size in SNM

        > they are different from "model params" (weights, bias, etc.)
        > Each type of ML model has its own set of HyperParams to tune
        > ex: C, kernel, gamma -> SVM
        > ex: alpha -> Lasso

        > To Implement:
            - basically just make a list of params + their value range (for each model)
            - feed this into GridSearchCV fn. & it'll give the best_params_
            - use "model.set_params()" to set the resulting best params into the model
            - & then "model.fit()" & so on....